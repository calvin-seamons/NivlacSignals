{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exceptions import DataPipelineError, FeatureEngineeringError\n",
    "\n",
    "from data.data_pipeline import DataPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class DataStatus:\n",
    "    raw_count: int\n",
    "    processed_count: int\n",
    "    universe_count: int\n",
    "    raw_symbols: list\n",
    "    processed_symbols: list\n",
    "    universe_symbols: list\n",
    "    columns: list\n",
    "\n",
    "def format_data_status(pipeline) -> str:\n",
    "    \"\"\"\n",
    "    Format the data status output in a readable table format.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: DataPipeline instance\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with data status information\n",
    "    \"\"\"\n",
    "    status = pipeline.check_data_status()\n",
    "    \n",
    "    # Create DataStatus object for easier handling\n",
    "    data_status = DataStatus(\n",
    "        raw_count=status['raw_data']['count'],\n",
    "        processed_count=status['processed_data']['count'],\n",
    "        universe_count=status['universe']['count'],\n",
    "        raw_symbols=status['raw_data']['symbols'],\n",
    "        processed_symbols=status['processed_data']['symbols'],\n",
    "        universe_symbols=status['universe']['symbols'],\n",
    "        columns=status['raw_data']['sample_columns'] if status['raw_data']['count'] > 0 else []\n",
    "    )\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = [\n",
    "        [\"Raw Data\", data_status.raw_count, \", \".join(data_status.raw_symbols[:3]) + (\"...\" if len(data_status.raw_symbols) > 3 else \"\")],\n",
    "        [\"Processed Data\", data_status.processed_count, \", \".join(data_status.processed_symbols[:3]) + (\"...\" if len(data_status.processed_symbols) > 3 else \"\")],\n",
    "        [\"Universe\", data_status.universe_count, \", \".join(data_status.universe_symbols[:3]) + (\"...\" if len(data_status.universe_symbols) > 3 else \"\")]\n",
    "    ]\n",
    "    \n",
    "    summary_table = tabulate(summary_data, \n",
    "                           headers=[\"Stage\", \"Count\", \"Sample Symbols\"],\n",
    "                           tablefmt=\"grid\")\n",
    "    \n",
    "    # Create columns table if available\n",
    "    columns_str = \"\"\n",
    "    if data_status.columns:\n",
    "        columns_table = tabulate([[\"Available Columns\", \", \".join(data_status.columns)]], \n",
    "                               tablefmt=\"grid\")\n",
    "        columns_str = f\"\\n\\nColumns:\\n{columns_table}\"\n",
    "    \n",
    "    return f\"Data Pipeline Status:\\n{summary_table}{columns_str}\"\n",
    "\n",
    "def format_data_quality(pipeline) -> str:\n",
    "    \"\"\"\n",
    "    Format the data quality metrics in a readable table format.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: DataPipeline instance\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with data quality information\n",
    "    \"\"\"\n",
    "    quality_metrics = pipeline.validate_data_quality()\n",
    "    \n",
    "    # Prepare data for main metrics table\n",
    "    metrics_data = []\n",
    "    for symbol, metrics in quality_metrics.items():\n",
    "        metrics_data.append([\n",
    "            symbol,\n",
    "            metrics['data_points'],\n",
    "            metrics['missing_values'],\n",
    "            f\"{metrics['avg_volume']:,.0f}\",\n",
    "            metrics['zero_volume_days'],\n",
    "            metrics['start_date'].strftime('%Y-%m-%d'),\n",
    "            metrics['end_date'].strftime('%Y-%m-%d')\n",
    "        ])\n",
    "    \n",
    "    # Create main metrics table\n",
    "    metrics_table = tabulate(metrics_data,\n",
    "                           headers=[\"Symbol\", \"Data Points\", \"Missing Values\", \n",
    "                                  \"Avg Volume\", \"Zero Volume Days\", \n",
    "                                  \"Start Date\", \"End Date\"],\n",
    "                           tablefmt=\"grid\")\n",
    "    \n",
    "    # Calculate and format summary statistics\n",
    "    total_data_points = sum(m['data_points'] for m in quality_metrics.values())\n",
    "    total_missing = sum(m['missing_values'] for m in quality_metrics.values())\n",
    "    avg_missing = total_missing / len(quality_metrics) if quality_metrics else 0\n",
    "    \n",
    "    summary_data = [\n",
    "        [\"Total Symbols\", len(quality_metrics)],\n",
    "        [\"Total Data Points\", total_data_points],\n",
    "        [\"Average Missing Values\", f\"{avg_missing:.2f}\"],\n",
    "        [\"Date Range\", f\"{min((m['start_date'] for m in quality_metrics.values())).strftime('%Y-%m-%d')} to \"\n",
    "                      f\"{max((m['end_date'] for m in quality_metrics.values())).strftime('%Y-%m-%d')}\"]\n",
    "    ]\n",
    "    \n",
    "    summary_table = tabulate(summary_data,\n",
    "                           headers=[\"Metric\", \"Value\"],\n",
    "                           tablefmt=\"grid\")\n",
    "    \n",
    "    return f\"Data Quality Summary:\\n{summary_table}\\n\\nDetailed Metrics by Symbol:\\n{metrics_table}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:43:42 - DataPipeline - INFO - Fetching data for 14 symbols...\n",
      "16:43:43 - DataPipeline - INFO - Successfully fetched data for 14 symbols\n",
      "\n",
      "Status after fetch:\n",
      "Data Pipeline Status:\n",
      "+----------------+---------+---------------------+\n",
      "| Stage          |   Count | Sample Symbols      |\n",
      "+================+=========+=====================+\n",
      "| Raw Data       |      14 | AMZN, TSLA, META... |\n",
      "+----------------+---------+---------------------+\n",
      "| Processed Data |       0 |                     |\n",
      "+----------------+---------+---------------------+\n",
      "| Universe       |       0 |                     |\n",
      "+----------------+---------+---------------------+\n",
      "\n",
      "Columns:\n",
      "+-------------------+---------------------------------------------------------+\n",
      "| Available Columns | Open, High, Low, Close, Volume, Dividends, Stock Splits |\n",
      "+-------------------+---------------------------------------------------------+\n",
      "16:43:43 - DataPipeline - INFO - Starting data processing with 14 symbols\n",
      "16:43:43 - DataPipeline - INFO - Processed data for 14 symbols\n",
      "\n",
      "Quality after processing:\n",
      "Data Quality Summary:\n",
      "+------------------------+--------------------------+\n",
      "| Metric                 | Value                    |\n",
      "+========================+==========================+\n",
      "| Total Symbols          | 14                       |\n",
      "+------------------------+--------------------------+\n",
      "| Total Data Points      | 48587                    |\n",
      "+------------------------+--------------------------+\n",
      "| Average Missing Values | 60.00                    |\n",
      "+------------------------+--------------------------+\n",
      "| Date Range             | 2010-01-04 to 2023-12-29 |\n",
      "+------------------------+--------------------------+\n",
      "\n",
      "Detailed Metrics by Symbol:\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| Symbol   |   Data Points |   Missing Values | Avg Volume   |   Zero Volume Days | Start Date   | End Date   |\n",
      "+==========+===============+==================+==============+====================+==============+============+\n",
      "| AMZN     |          3522 |               60 | 86,238,044   |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| TSLA     |          3400 |               60 | 96,811,665   |                  0 | 2010-06-29   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| META     |          2923 |               60 | 30,604,725   |                  0 | 2012-05-18   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| JPM      |          3522 |               60 | 20,001,582   |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| BAC      |          3522 |               60 | 100,680,640  |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| XOM      |          3522 |               60 | 17,579,803   |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| NVDA     |          3522 |               60 | 505,556,007  |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| PG       |          3522 |               60 | 8,903,353    |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| JNJ      |          3522 |               60 | 8,788,027    |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| WMT      |          3522 |               60 | 26,303,760   |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| HD       |          3522 |               60 | 6,489,105    |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| V        |          3522 |               60 | 11,572,262   |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| COST     |          3522 |               60 | 2,423,384    |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| DIS      |          3522 |               60 | 9,997,458    |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "16:43:43 - DataPipeline - INFO - Created universe with 14 symbols\n",
      "Total feature columns: 59\n",
      "Unique feature columns: ['Dividends', 'High', 'Low', 'Open', 'Stock Splits', 'log_returns', 'macd', 'macd_signal', 'return_lag_1', 'return_lag_10', 'return_lag_11', 'return_lag_12', 'return_lag_13', 'return_lag_14', 'return_lag_15', 'return_lag_16', 'return_lag_17', 'return_lag_18', 'return_lag_19', 'return_lag_2', 'return_lag_20', 'return_lag_3', 'return_lag_4', 'return_lag_5', 'return_lag_6', 'return_lag_7', 'return_lag_8', 'return_lag_9', 'return_mean_5d', 'return_std_5d', 'returns', 'rsi', 'sma_10', 'sma_30', 'sma_60', 'volatility', 'volume_lag_1', 'volume_lag_10', 'volume_lag_11', 'volume_lag_12', 'volume_lag_13', 'volume_lag_14', 'volume_lag_15', 'volume_lag_16', 'volume_lag_17', 'volume_lag_18', 'volume_lag_19', 'volume_lag_2', 'volume_lag_20', 'volume_lag_3', 'volume_lag_4', 'volume_lag_5', 'volume_lag_6', 'volume_lag_7', 'volume_lag_8', 'volume_lag_9', 'volume_ma', 'volume_mean_5d', 'volume_std']\n",
      "Total feature columns: 59\n",
      "Unique feature columns: ['Dividends', 'High', 'Low', 'Open', 'Stock Splits', 'log_returns', 'macd', 'macd_signal', 'return_lag_1', 'return_lag_10', 'return_lag_11', 'return_lag_12', 'return_lag_13', 'return_lag_14', 'return_lag_15', 'return_lag_16', 'return_lag_17', 'return_lag_18', 'return_lag_19', 'return_lag_2', 'return_lag_20', 'return_lag_3', 'return_lag_4', 'return_lag_5', 'return_lag_6', 'return_lag_7', 'return_lag_8', 'return_lag_9', 'return_mean_5d', 'return_std_5d', 'returns', 'rsi', 'sma_10', 'sma_30', 'sma_60', 'volatility', 'volume_lag_1', 'volume_lag_10', 'volume_lag_11', 'volume_lag_12', 'volume_lag_13', 'volume_lag_14', 'volume_lag_15', 'volume_lag_16', 'volume_lag_17', 'volume_lag_18', 'volume_lag_19', 'volume_lag_2', 'volume_lag_20', 'volume_lag_3', 'volume_lag_4', 'volume_lag_5', 'volume_lag_6', 'volume_lag_7', 'volume_lag_8', 'volume_lag_9', 'volume_ma', 'volume_mean_5d', 'volume_std']\n",
      "Total feature columns: 59\n",
      "Unique feature columns: ['Dividends', 'High', 'Low', 'Open', 'Stock Splits', 'log_returns', 'macd', 'macd_signal', 'return_lag_1', 'return_lag_10', 'return_lag_11', 'return_lag_12', 'return_lag_13', 'return_lag_14', 'return_lag_15', 'return_lag_16', 'return_lag_17', 'return_lag_18', 'return_lag_19', 'return_lag_2', 'return_lag_20', 'return_lag_3', 'return_lag_4', 'return_lag_5', 'return_lag_6', 'return_lag_7', 'return_lag_8', 'return_lag_9', 'return_mean_5d', 'return_std_5d', 'returns', 'rsi', 'sma_10', 'sma_30', 'sma_60', 'volatility', 'volume_lag_1', 'volume_lag_10', 'volume_lag_11', 'volume_lag_12', 'volume_lag_13', 'volume_lag_14', 'volume_lag_15', 'volume_lag_16', 'volume_lag_17', 'volume_lag_18', 'volume_lag_19', 'volume_lag_2', 'volume_lag_20', 'volume_lag_3', 'volume_lag_4', 'volume_lag_5', 'volume_lag_6', 'volume_lag_7', 'volume_lag_8', 'volume_lag_9', 'volume_ma', 'volume_mean_5d', 'volume_std']\n"
     ]
    }
   ],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = DataPipeline(\n",
    "    start_date='2010-01-01',\n",
    "    end_date='2023-12-31',\n",
    "    universe_size=500,\n",
    "    cache_dir='data/cache',\n",
    "    price_col='Close'\n",
    ")\n",
    "\n",
    "# Fetch data\n",
    "symbols = [\n",
    "    'AMZN', 'META', 'NVDA', 'TSLA', 'JPM', 'JNJ', 'WMT', \n",
    "    'PG', 'XOM', 'BAC', 'HD', 'COST', 'V', 'DIS'\n",
    "]\n",
    "pipeline.fetch_data(symbols)\n",
    "\n",
    "# Check status after fetch\n",
    "print(\"\\nStatus after fetch:\")\n",
    "print(format_data_status(pipeline))\n",
    "\n",
    "# Process data\n",
    "pipeline.process_data()\n",
    "\n",
    "# Check quality after processing\n",
    "print(\"\\nQuality after processing:\")\n",
    "print(format_data_quality(pipeline))\n",
    "\n",
    "# Create universe\n",
    "pipeline.create_universe()\n",
    "\n",
    "X_train, X_test, y_train, y_test = pipeline.get_training_data()\n",
    "\n",
    "print(\"\\nX_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:42:21 - DataPipeline - INFO - Filtered universe contains 1 symbols\n",
      "Mean Reversion Positions:\n",
      "----------------------------------------\n",
      "Long positions (0): []\n",
      "Short positions (0): []\n"
     ]
    }
   ],
   "source": [
    "from models.mean_reversion_analyzer import MeanReversionAnalyzer\n",
    "\n",
    "# Initialize mean reversion analyzer with our existing pipeline\n",
    "mean_reversion = MeanReversionAnalyzer(\n",
    "    data_pipeline=pipeline,\n",
    "    lookback_periods=20,\n",
    "    z_score_threshold=2.0,\n",
    "    volume_percentile=0.7,\n",
    "    max_positions=5\n",
    ")\n",
    "\n",
    "# Generate signals and store them\n",
    "signals = mean_reversion.generate_signals()\n",
    "\n",
    "# Display initial results\n",
    "print(\"Mean Reversion Positions:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Long positions ({len(signals.longs)}):\", signals.longs)\n",
    "print(f\"Short positions ({len(signals.shorts)}):\", signals.shorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current universe: ['AAPL', 'GOOGL', 'MSFT']\n",
      "\n",
      "Filtered universe: ['AAPL']\n",
      "\n",
      "Z-scores for AAPL:\n",
      "Latest z-score: -0.788\n",
      "\n",
      "Z-scores for GOOGL:\n",
      "Latest z-score: 1.007\n",
      "\n",
      "Z-scores for MSFT:\n",
      "Latest z-score: 1.344\n"
     ]
    }
   ],
   "source": [
    "# First, let's check what our universe looks like\n",
    "print(\"Current universe:\", pipeline.universe)\n",
    "\n",
    "# Let's examine the filtering process\n",
    "filtered_universe = mean_reversion.filter_universe()\n",
    "print(\"\\nFiltered universe:\", filtered_universe)\n",
    "\n",
    "# Let's look at the actual z-scores before thresholding\n",
    "for symbol in pipeline.universe:\n",
    "    data = pipeline.processed_data[symbol]\n",
    "    z_scores = mean_reversion.calculate_z_scores(data)\n",
    "    print(f\"\\nZ-scores for {symbol}:\")\n",
    "    print(f\"Latest z-score: {z_scores.iloc[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:42:39 - DataPipeline - INFO - Initializing model pipeline with xgboost model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\calvi\\anaconda3\\envs\\ns_env\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:42:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"early_stopping_rounds\", \"n_estimators\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:42:40 - DataPipeline - INFO - Model Evaluation Results:\n",
      "15:42:41 - DataPipeline - INFO - mse: 0.0015\n",
      "15:42:41 - DataPipeline - INFO - rmse: 0.0387\n",
      "15:42:41 - DataPipeline - INFO - mae: 0.0300\n",
      "15:42:41 - DataPipeline - INFO - r2: -0.0121\n",
      "15:42:41 - DataPipeline - INFO - directional_accuracy: 0.5556\n",
      "15:42:41 - DataPipeline - INFO - precision: 0.5579\n",
      "15:42:41 - DataPipeline - INFO - recall: 0.9607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mse': 0.0014997024404333568,\n",
       " 'rmse': 0.03872599179405683,\n",
       " 'mae': 0.029974383256972565,\n",
       " 'r2': -0.012068691908382778,\n",
       " 'directional_accuracy': 0.5555555555555556,\n",
       " 'precision': 0.5578680203045685,\n",
       " 'recall': 0.9606643356643356}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "from models.model_pipeline import ModelPipeline\n",
    "\n",
    "# Initialize pipeline\n",
    "model = ModelPipeline()\n",
    "\n",
    "# Train model\n",
    "model.train(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "model.evaluate_model(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ns_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
