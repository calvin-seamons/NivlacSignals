{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_pipeline import DataPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class DataStatus:\n",
    "    raw_count: int\n",
    "    processed_count: int\n",
    "    universe_count: int\n",
    "    raw_symbols: list\n",
    "    processed_symbols: list\n",
    "    universe_symbols: list\n",
    "    columns: list\n",
    "\n",
    "def format_data_status(pipeline) -> str:\n",
    "    \"\"\"\n",
    "    Format the data status output in a readable table format.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: DataPipeline instance\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with data status information\n",
    "    \"\"\"\n",
    "    status = pipeline.check_data_status()\n",
    "    \n",
    "    # Create DataStatus object for easier handling\n",
    "    data_status = DataStatus(\n",
    "        raw_count=status['raw_data']['count'],\n",
    "        processed_count=status['processed_data']['count'],\n",
    "        universe_count=status['universe']['count'],\n",
    "        raw_symbols=status['raw_data']['symbols'],\n",
    "        processed_symbols=status['processed_data']['symbols'],\n",
    "        universe_symbols=status['universe']['symbols'],\n",
    "        columns=status['raw_data']['sample_columns'] if status['raw_data']['count'] > 0 else []\n",
    "    )\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = [\n",
    "        [\"Raw Data\", data_status.raw_count, \", \".join(data_status.raw_symbols[:3]) + (\"...\" if len(data_status.raw_symbols) > 3 else \"\")],\n",
    "        [\"Processed Data\", data_status.processed_count, \", \".join(data_status.processed_symbols[:3]) + (\"...\" if len(data_status.processed_symbols) > 3 else \"\")],\n",
    "        [\"Universe\", data_status.universe_count, \", \".join(data_status.universe_symbols[:3]) + (\"...\" if len(data_status.universe_symbols) > 3 else \"\")]\n",
    "    ]\n",
    "    \n",
    "    summary_table = tabulate(summary_data, \n",
    "                           headers=[\"Stage\", \"Count\", \"Sample Symbols\"],\n",
    "                           tablefmt=\"grid\")\n",
    "    \n",
    "    # Create columns table if available\n",
    "    columns_str = \"\"\n",
    "    if data_status.columns:\n",
    "        columns_table = tabulate([[\"Available Columns\", \", \".join(data_status.columns)]], \n",
    "                               tablefmt=\"grid\")\n",
    "        columns_str = f\"\\n\\nColumns:\\n{columns_table}\"\n",
    "    \n",
    "    return f\"Data Pipeline Status:\\n{summary_table}{columns_str}\"\n",
    "\n",
    "def format_data_quality(pipeline) -> str:\n",
    "    \"\"\"\n",
    "    Format the data quality metrics in a readable table format.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: DataPipeline instance\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with data quality information\n",
    "    \"\"\"\n",
    "    quality_metrics = pipeline.validate_data_quality()\n",
    "    \n",
    "    # Prepare data for main metrics table\n",
    "    metrics_data = []\n",
    "    for symbol, metrics in quality_metrics.items():\n",
    "        metrics_data.append([\n",
    "            symbol,\n",
    "            metrics['data_points'],\n",
    "            metrics['missing_values'],\n",
    "            f\"{metrics['avg_volume']:,.0f}\",\n",
    "            metrics['zero_volume_days'],\n",
    "            metrics['start_date'].strftime('%Y-%m-%d'),\n",
    "            metrics['end_date'].strftime('%Y-%m-%d')\n",
    "        ])\n",
    "    \n",
    "    # Create main metrics table\n",
    "    metrics_table = tabulate(metrics_data,\n",
    "                           headers=[\"Symbol\", \"Data Points\", \"Missing Values\", \n",
    "                                  \"Avg Volume\", \"Zero Volume Days\", \n",
    "                                  \"Start Date\", \"End Date\"],\n",
    "                           tablefmt=\"grid\")\n",
    "    \n",
    "    # Calculate and format summary statistics\n",
    "    total_data_points = sum(m['data_points'] for m in quality_metrics.values())\n",
    "    total_missing = sum(m['missing_values'] for m in quality_metrics.values())\n",
    "    avg_missing = total_missing / len(quality_metrics) if quality_metrics else 0\n",
    "    \n",
    "    summary_data = [\n",
    "        [\"Total Symbols\", len(quality_metrics)],\n",
    "        [\"Total Data Points\", total_data_points],\n",
    "        [\"Average Missing Values\", f\"{avg_missing:.2f}\"],\n",
    "        [\"Date Range\", f\"{min((m['start_date'] for m in quality_metrics.values())).strftime('%Y-%m-%d')} to \"\n",
    "                      f\"{max((m['end_date'] for m in quality_metrics.values())).strftime('%Y-%m-%d')}\"]\n",
    "    ]\n",
    "    \n",
    "    summary_table = tabulate(summary_data,\n",
    "                           headers=[\"Metric\", \"Value\"],\n",
    "                           tablefmt=\"grid\")\n",
    "    \n",
    "    return f\"Data Quality Summary:\\n{summary_table}\\n\\nDetailed Metrics by Symbol:\\n{metrics_table}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:44:14 - DataPipeline - INFO - Fetching data for 3 symbols...\n",
      "16:44:14 - DataPipeline - INFO - Successfully fetched data for 3 symbols\n",
      "\n",
      "Status after fetch:\n",
      "Data Pipeline Status:\n",
      "+----------------+---------+-------------------+\n",
      "| Stage          |   Count | Sample Symbols    |\n",
      "+================+=========+===================+\n",
      "| Raw Data       |       3 | AAPL, GOOGL, MSFT |\n",
      "+----------------+---------+-------------------+\n",
      "| Processed Data |       0 |                   |\n",
      "+----------------+---------+-------------------+\n",
      "| Universe       |       0 |                   |\n",
      "+----------------+---------+-------------------+\n",
      "\n",
      "Columns:\n",
      "+-------------------+---------------------------------------------------------+\n",
      "| Available Columns | Open, High, Low, Close, Volume, Dividends, Stock Splits |\n",
      "+-------------------+---------------------------------------------------------+\n",
      "16:44:14 - DataPipeline - INFO - Starting data processing with 3 symbols\n",
      "16:44:14 - DataPipeline - INFO - Processed data for 3 symbols\n",
      "\n",
      "Quality after processing:\n",
      "Data Quality Summary:\n",
      "+------------------------+--------------------------+\n",
      "| Metric                 | Value                    |\n",
      "+========================+==========================+\n",
      "| Total Symbols          | 3                        |\n",
      "+------------------------+--------------------------+\n",
      "| Total Data Points      | 10566                    |\n",
      "+------------------------+--------------------------+\n",
      "| Average Missing Values | 60.00                    |\n",
      "+------------------------+--------------------------+\n",
      "| Date Range             | 2010-01-04 to 2023-12-29 |\n",
      "+------------------------+--------------------------+\n",
      "\n",
      "Detailed Metrics by Symbol:\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| Symbol   |   Data Points |   Missing Values | Avg Volume   |   Zero Volume Days | Start Date   | End Date   |\n",
      "+==========+===============+==================+==============+====================+==============+============+\n",
      "| AAPL     |          3522 |               60 | 242,283,396  |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| GOOGL    |          3522 |               60 | 58,227,074   |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| MSFT     |          3522 |               60 | 37,277,882   |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "16:44:14 - DataPipeline - INFO - Created universe with 3 symbols\n",
      "16:44:56 - DataPipeline - INFO - Training data from 2010-04-28 00:00:00-04:00 to 2021-03-31 00:00:00-04:00\n",
      "16:44:56 - DataPipeline - INFO - Testing data from 2021-03-31 00:00:00-04:00 to 2023-12-21 00:00:00-05:00\n",
      "16:44:56 - DataPipeline - INFO - Training samples: 8253, Testing samples: 2061\n",
      "\n",
      "X_train shape: (8253, 1180)\n",
      "X_test shape: (2061, 1180)\n"
     ]
    }
   ],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = DataPipeline(\n",
    "    start_date='2010-01-01',\n",
    "    end_date='2023-12-31',\n",
    "    universe_size=500,\n",
    "    cache_dir='data/cache',\n",
    "    price_col='Close'\n",
    ")\n",
    "\n",
    "# Fetch data\n",
    "symbols = ['AAPL', 'GOOGL', 'MSFT']\n",
    "pipeline.fetch_data(symbols)\n",
    "\n",
    "# Check status after fetch\n",
    "print(\"\\nStatus after fetch:\")\n",
    "print(format_data_status(pipeline))\n",
    "\n",
    "# Process data\n",
    "pipeline.process_data()\n",
    "\n",
    "# Check quality after processing\n",
    "print(\"\\nQuality after processing:\")\n",
    "print(format_data_quality(pipeline))\n",
    "\n",
    "# Create universe\n",
    "pipeline.create_universe()\n",
    "\n",
    "X_train, X_test, y_train, y_test = pipeline.get_training_data()\n",
    "\n",
    "print(\"\\nX_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "feature_names = pipeline.feature_engineer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_train:\n",
      "0   -0.021445\n",
      "1   -0.036716\n",
      "2   -0.034293\n",
      "3   -0.083345\n",
      "4   -0.062650\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 rows of y_train\n",
    "print(\"\\ny_train:\")\n",
    "print(pd.Series(y_train).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:45:00 - DataPipeline - INFO - Initializing model pipeline with xgboost model\n",
      "16:45:11 - DataPipeline - WARNING - Error calculating feature importance: `feature_names_in_` is defined only when `X` has feature names that are all strings.\n",
      "16:45:11 - DataPipeline - INFO - Model Evaluation Results:\n",
      "16:45:11 - DataPipeline - INFO - mse: 0.0015\n",
      "16:45:11 - DataPipeline - INFO - rmse: 0.0385\n",
      "16:45:11 - DataPipeline - INFO - mae: 0.0298\n",
      "16:45:11 - DataPipeline - INFO - r2: -0.0015\n",
      "16:45:11 - DataPipeline - INFO - directional_accuracy: 0.5556\n",
      "16:45:11 - DataPipeline - INFO - precision: 0.5558\n",
      "16:45:11 - DataPipeline - INFO - recall: 0.9930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mse': 0.0014841001949252991,\n",
       " 'rmse': 0.03852402101189983,\n",
       " 'mae': 0.029827279521793145,\n",
       " 'r2': -0.0015395308748367142,\n",
       " 'directional_accuracy': 0.5555555555555556,\n",
       " 'precision': 0.5557729941291585,\n",
       " 'recall': 0.993006993006993}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "from models.model_pipeline import ModelPipeline\n",
    "\n",
    "# Initialize pipeline\n",
    "model = ModelPipeline()\n",
    "\n",
    "# Train model\n",
    "model.train(X_train, y_train, X_test, y_test, feature_names=feature_names)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "model.evaluate_model(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ns_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
