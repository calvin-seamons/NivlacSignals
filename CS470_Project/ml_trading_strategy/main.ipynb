{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exceptions import DataPipelineError, FeatureEngineeringError\n",
    "\n",
    "from data.data_pipeline import DataPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class DataStatus:\n",
    "    raw_count: int\n",
    "    processed_count: int\n",
    "    universe_count: int\n",
    "    raw_symbols: list\n",
    "    processed_symbols: list\n",
    "    universe_symbols: list\n",
    "    columns: list\n",
    "\n",
    "def format_data_status(pipeline) -> str:\n",
    "    \"\"\"\n",
    "    Format the data status output in a readable table format.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: DataPipeline instance\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with data status information\n",
    "    \"\"\"\n",
    "    status = pipeline.check_data_status()\n",
    "    \n",
    "    # Create DataStatus object for easier handling\n",
    "    data_status = DataStatus(\n",
    "        raw_count=status['raw_data']['count'],\n",
    "        processed_count=status['processed_data']['count'],\n",
    "        universe_count=status['universe']['count'],\n",
    "        raw_symbols=status['raw_data']['symbols'],\n",
    "        processed_symbols=status['processed_data']['symbols'],\n",
    "        universe_symbols=status['universe']['symbols'],\n",
    "        columns=status['raw_data']['sample_columns'] if status['raw_data']['count'] > 0 else []\n",
    "    )\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = [\n",
    "        [\"Raw Data\", data_status.raw_count, \", \".join(data_status.raw_symbols[:3]) + (\"...\" if len(data_status.raw_symbols) > 3 else \"\")],\n",
    "        [\"Processed Data\", data_status.processed_count, \", \".join(data_status.processed_symbols[:3]) + (\"...\" if len(data_status.processed_symbols) > 3 else \"\")],\n",
    "        [\"Universe\", data_status.universe_count, \", \".join(data_status.universe_symbols[:3]) + (\"...\" if len(data_status.universe_symbols) > 3 else \"\")]\n",
    "    ]\n",
    "    \n",
    "    summary_table = tabulate(summary_data, \n",
    "                           headers=[\"Stage\", \"Count\", \"Sample Symbols\"],\n",
    "                           tablefmt=\"grid\")\n",
    "    \n",
    "    # Create columns table if available\n",
    "    columns_str = \"\"\n",
    "    if data_status.columns:\n",
    "        columns_table = tabulate([[\"Available Columns\", \", \".join(data_status.columns)]], \n",
    "                               tablefmt=\"grid\")\n",
    "        columns_str = f\"\\n\\nColumns:\\n{columns_table}\"\n",
    "    \n",
    "    return f\"Data Pipeline Status:\\n{summary_table}{columns_str}\"\n",
    "\n",
    "def format_data_quality(pipeline) -> str:\n",
    "    \"\"\"\n",
    "    Format the data quality metrics in a readable table format.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: DataPipeline instance\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with data quality information\n",
    "    \"\"\"\n",
    "    quality_metrics = pipeline.validate_data_quality()\n",
    "    \n",
    "    # Prepare data for main metrics table\n",
    "    metrics_data = []\n",
    "    for symbol, metrics in quality_metrics.items():\n",
    "        metrics_data.append([\n",
    "            symbol,\n",
    "            metrics['data_points'],\n",
    "            metrics['missing_values'],\n",
    "            f\"{metrics['avg_volume']:,.0f}\",\n",
    "            metrics['zero_volume_days'],\n",
    "            metrics['start_date'].strftime('%Y-%m-%d'),\n",
    "            metrics['end_date'].strftime('%Y-%m-%d')\n",
    "        ])\n",
    "    \n",
    "    # Create main metrics table\n",
    "    metrics_table = tabulate(metrics_data,\n",
    "                           headers=[\"Symbol\", \"Data Points\", \"Missing Values\", \n",
    "                                  \"Avg Volume\", \"Zero Volume Days\", \n",
    "                                  \"Start Date\", \"End Date\"],\n",
    "                           tablefmt=\"grid\")\n",
    "    \n",
    "    # Calculate and format summary statistics\n",
    "    total_data_points = sum(m['data_points'] for m in quality_metrics.values())\n",
    "    total_missing = sum(m['missing_values'] for m in quality_metrics.values())\n",
    "    avg_missing = total_missing / len(quality_metrics) if quality_metrics else 0\n",
    "    \n",
    "    summary_data = [\n",
    "        [\"Total Symbols\", len(quality_metrics)],\n",
    "        [\"Total Data Points\", total_data_points],\n",
    "        [\"Average Missing Values\", f\"{avg_missing:.2f}\"],\n",
    "        [\"Date Range\", f\"{min((m['start_date'] for m in quality_metrics.values())).strftime('%Y-%m-%d')} to \"\n",
    "                      f\"{max((m['end_date'] for m in quality_metrics.values())).strftime('%Y-%m-%d')}\"]\n",
    "    ]\n",
    "    \n",
    "    summary_table = tabulate(summary_data,\n",
    "                           headers=[\"Metric\", \"Value\"],\n",
    "                           tablefmt=\"grid\")\n",
    "    \n",
    "    return f\"Data Quality Summary:\\n{summary_table}\\n\\nDetailed Metrics by Symbol:\\n{metrics_table}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:47:32 - DataPipeline - INFO - Fetching data for 14 symbols...\n",
      "19:47:32 - DataPipeline - INFO - Successfully fetched data for 14 symbols\n",
      "\n",
      "Status after fetch:\n",
      "Data Pipeline Status:\n",
      "+----------------+---------+---------------------+\n",
      "| Stage          |   Count | Sample Symbols      |\n",
      "+================+=========+=====================+\n",
      "| Raw Data       |      14 | TSLA, NVDA, META... |\n",
      "+----------------+---------+---------------------+\n",
      "| Processed Data |       0 |                     |\n",
      "+----------------+---------+---------------------+\n",
      "| Universe       |       0 |                     |\n",
      "+----------------+---------+---------------------+\n",
      "\n",
      "Columns:\n",
      "+-------------------+---------------------------------------------------------+\n",
      "| Available Columns | Open, High, Low, Close, Volume, Dividends, Stock Splits |\n",
      "+-------------------+---------------------------------------------------------+\n",
      "19:47:32 - DataPipeline - INFO - Starting data processing with 14 symbols\n",
      "19:47:32 - DataPipeline - INFO - Processed data for 14 symbols\n",
      "\n",
      "Quality after processing:\n",
      "Data Quality Summary:\n",
      "+------------------------+--------------------------+\n",
      "| Metric                 | Value                    |\n",
      "+========================+==========================+\n",
      "| Total Symbols          | 14                       |\n",
      "+------------------------+--------------------------+\n",
      "| Total Data Points      | 48587                    |\n",
      "+------------------------+--------------------------+\n",
      "| Average Missing Values | 60.00                    |\n",
      "+------------------------+--------------------------+\n",
      "| Date Range             | 2010-01-04 to 2023-12-29 |\n",
      "+------------------------+--------------------------+\n",
      "\n",
      "Detailed Metrics by Symbol:\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| Symbol   |   Data Points |   Missing Values | Avg Volume   |   Zero Volume Days | Start Date   | End Date   |\n",
      "+==========+===============+==================+==============+====================+==============+============+\n",
      "| TSLA     |          3400 |               60 | 96,811,665   |                  0 | 2010-06-29   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| NVDA     |          3522 |               60 | 505,556,007  |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| META     |          2923 |               60 | 30,604,725   |                  0 | 2012-05-18   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| AMZN     |          3522 |               60 | 86,238,044   |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| JPM      |          3522 |               60 | 20,001,582   |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| JNJ      |          3522 |               60 | 8,788,027    |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| WMT      |          3522 |               60 | 26,303,760   |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| PG       |          3522 |               60 | 8,903,353    |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| XOM      |          3522 |               60 | 17,579,803   |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| BAC      |          3522 |               60 | 100,680,640  |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| HD       |          3522 |               60 | 6,489,105    |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| COST     |          3522 |               60 | 2,423,384    |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| V        |          3522 |               60 | 11,572,262   |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "| DIS      |          3522 |               60 | 9,997,458    |                  0 | 2010-01-04   | 2023-12-29 |\n",
      "+----------+---------------+------------------+--------------+--------------------+--------------+------------+\n",
      "19:47:32 - DataPipeline - INFO - Created universe with 14 symbols\n",
      "\n",
      "Status after creating universe:\n",
      "19:47:32 - DataPipeline - INFO - Starting get_training_data...\n",
      "19:47:32 - DataPipeline - INFO - Preparing features and targets...\n",
      "19:47:32 - DataPipeline - INFO - Starting feature preparation with 14 symbols\n",
      "19:47:32 - DataPipeline - INFO - Processing symbol NVDA (1/14)\n",
      "19:47:32 - DataPipeline - INFO - NVDA: Processing 3497 potential samples\n",
      "19:47:34 - DataPipeline - INFO - NVDA: Processed 1000 samples\n",
      "19:47:38 - DataPipeline - INFO - NVDA: Processed 2000 samples\n",
      "19:47:40 - DataPipeline - INFO - NVDA: Processed 3000 samples\n",
      "19:47:41 - DataPipeline - INFO - Completed NVDA with 3438 valid samples\n",
      "19:47:41 - DataPipeline - INFO - Processing symbol BAC (2/14)\n",
      "19:47:41 - DataPipeline - INFO - BAC: Processing 3497 potential samples\n",
      "19:47:43 - DataPipeline - INFO - BAC: Processed 1000 samples\n",
      "19:47:45 - DataPipeline - INFO - BAC: Processed 2000 samples\n",
      "19:47:47 - DataPipeline - INFO - BAC: Processed 3000 samples\n",
      "19:47:48 - DataPipeline - INFO - Completed BAC with 3438 valid samples\n",
      "19:47:48 - DataPipeline - INFO - Processing symbol TSLA (3/14)\n",
      "19:47:48 - DataPipeline - INFO - TSLA: Processing 3375 potential samples\n",
      "19:47:50 - DataPipeline - INFO - TSLA: Processed 1000 samples\n",
      "19:47:52 - DataPipeline - INFO - TSLA: Processed 2000 samples\n",
      "19:47:53 - DataPipeline - INFO - TSLA: Processed 3000 samples\n",
      "19:47:54 - DataPipeline - INFO - Completed TSLA with 3316 valid samples\n",
      "19:47:54 - DataPipeline - INFO - Processing symbol AMZN (4/14)\n",
      "19:47:54 - DataPipeline - INFO - AMZN: Processing 3497 potential samples\n",
      "19:47:56 - DataPipeline - INFO - AMZN: Processed 1000 samples\n",
      "19:47:58 - DataPipeline - INFO - AMZN: Processed 2000 samples\n",
      "19:48:00 - DataPipeline - INFO - AMZN: Processed 3000 samples\n",
      "19:48:01 - DataPipeline - INFO - Completed AMZN with 3438 valid samples\n",
      "19:48:01 - DataPipeline - INFO - Processing symbol META (5/14)\n",
      "19:48:01 - DataPipeline - INFO - META: Processing 2898 potential samples\n",
      "19:48:03 - DataPipeline - INFO - META: Processed 1000 samples\n",
      "19:48:05 - DataPipeline - INFO - META: Processed 2000 samples\n",
      "19:48:07 - DataPipeline - INFO - Completed META with 2839 valid samples\n",
      "19:48:07 - DataPipeline - INFO - Processing symbol WMT (6/14)\n",
      "19:48:07 - DataPipeline - INFO - WMT: Processing 3497 potential samples\n",
      "19:48:09 - DataPipeline - INFO - WMT: Processed 1000 samples\n",
      "19:48:11 - DataPipeline - INFO - WMT: Processed 2000 samples\n",
      "19:48:13 - DataPipeline - INFO - WMT: Processed 3000 samples\n",
      "19:48:13 - DataPipeline - INFO - Completed WMT with 3438 valid samples\n",
      "19:48:13 - DataPipeline - INFO - Processing symbol JPM (7/14)\n",
      "19:48:13 - DataPipeline - INFO - JPM: Processing 3497 potential samples\n",
      "19:48:15 - DataPipeline - INFO - JPM: Processed 1000 samples\n",
      "19:48:17 - DataPipeline - INFO - JPM: Processed 2000 samples\n",
      "19:48:19 - DataPipeline - INFO - JPM: Processed 3000 samples\n",
      "19:48:20 - DataPipeline - INFO - Completed JPM with 3438 valid samples\n",
      "19:48:20 - DataPipeline - INFO - Processing symbol XOM (8/14)\n",
      "19:48:20 - DataPipeline - INFO - XOM: Processing 3497 potential samples\n",
      "19:48:22 - DataPipeline - INFO - XOM: Processed 1000 samples\n",
      "19:48:24 - DataPipeline - INFO - XOM: Processed 2000 samples\n",
      "19:48:26 - DataPipeline - INFO - XOM: Processed 3000 samples\n",
      "19:48:27 - DataPipeline - INFO - Completed XOM with 3438 valid samples\n",
      "19:48:27 - DataPipeline - INFO - Processing symbol V (9/14)\n",
      "19:48:27 - DataPipeline - INFO - V: Processing 3497 potential samples\n",
      "19:48:29 - DataPipeline - INFO - V: Processed 1000 samples\n",
      "19:48:31 - DataPipeline - INFO - V: Processed 2000 samples\n",
      "19:48:33 - DataPipeline - INFO - V: Processed 3000 samples\n",
      "19:48:34 - DataPipeline - INFO - Completed V with 3438 valid samples\n",
      "19:48:34 - DataPipeline - INFO - Processing symbol DIS (10/14)\n",
      "19:48:34 - DataPipeline - INFO - DIS: Processing 3497 potential samples\n",
      "19:48:36 - DataPipeline - INFO - DIS: Processed 1000 samples\n",
      "19:48:38 - DataPipeline - INFO - DIS: Processed 2000 samples\n",
      "19:48:39 - DataPipeline - INFO - DIS: Processed 3000 samples\n",
      "19:48:40 - DataPipeline - INFO - Completed DIS with 3438 valid samples\n",
      "19:48:40 - DataPipeline - INFO - Processing symbol PG (11/14)\n",
      "19:48:40 - DataPipeline - INFO - PG: Processing 3497 potential samples\n",
      "19:48:42 - DataPipeline - INFO - PG: Processed 1000 samples\n",
      "19:48:44 - DataPipeline - INFO - PG: Processed 2000 samples\n",
      "19:48:46 - DataPipeline - INFO - PG: Processed 3000 samples\n",
      "19:48:47 - DataPipeline - INFO - Completed PG with 3438 valid samples\n",
      "19:48:47 - DataPipeline - INFO - Processing symbol JNJ (12/14)\n",
      "19:48:47 - DataPipeline - INFO - JNJ: Processing 3497 potential samples\n",
      "19:48:49 - DataPipeline - INFO - JNJ: Processed 1000 samples\n",
      "19:48:51 - DataPipeline - INFO - JNJ: Processed 2000 samples\n",
      "19:48:53 - DataPipeline - INFO - JNJ: Processed 3000 samples\n",
      "19:48:54 - DataPipeline - INFO - Completed JNJ with 3438 valid samples\n",
      "19:48:54 - DataPipeline - INFO - Processing symbol HD (13/14)\n",
      "19:48:54 - DataPipeline - INFO - HD: Processing 3497 potential samples\n",
      "19:48:56 - DataPipeline - INFO - HD: Processed 1000 samples\n",
      "19:48:58 - DataPipeline - INFO - HD: Processed 2000 samples\n",
      "19:49:00 - DataPipeline - INFO - HD: Processed 3000 samples\n",
      "19:49:01 - DataPipeline - INFO - Completed HD with 3438 valid samples\n",
      "19:49:01 - DataPipeline - INFO - Processing symbol COST (14/14)\n",
      "19:49:01 - DataPipeline - INFO - COST: Processing 3497 potential samples\n",
      "19:49:03 - DataPipeline - INFO - COST: Processed 1000 samples\n",
      "19:49:05 - DataPipeline - INFO - COST: Processed 2000 samples\n",
      "19:49:06 - DataPipeline - INFO - COST: Processed 3000 samples\n",
      "19:49:07 - DataPipeline - INFO - Completed COST with 3438 valid samples\n",
      "19:49:07 - DataPipeline - INFO - Feature generation complete. Total samples: 47411\n",
      "19:49:07 - DataPipeline - INFO - Creating features array...\n",
      "19:49:07 - DataPipeline - INFO - Creating features DataFrame...\n",
      "19:49:08 - DataPipeline - INFO - Creating targets series...\n",
      "19:49:08 - DataPipeline - INFO - Applying normalization...\n",
      "19:49:08 - DataPipeline - INFO - Starting normalization for DataFrame of shape (47411, 1180)\n",
      "19:49:08 - DataPipeline - INFO - Processing 3438 unique dates\n",
      "19:49:08 - DataPipeline - INFO - Processed 0/3438 dates\n",
      "19:49:08 - DataPipeline - INFO - Processed 100/3438 dates\n",
      "19:49:08 - DataPipeline - INFO - Processed 200/3438 dates\n",
      "19:49:08 - DataPipeline - INFO - Processed 300/3438 dates\n",
      "19:49:08 - DataPipeline - INFO - Processed 400/3438 dates\n",
      "19:49:08 - DataPipeline - INFO - Processed 500/3438 dates\n",
      "19:49:08 - DataPipeline - INFO - Processed 600/3438 dates\n",
      "19:49:08 - DataPipeline - INFO - Processed 700/3438 dates\n",
      "19:49:08 - DataPipeline - INFO - Processed 800/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 900/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 1000/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 1100/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 1200/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 1300/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 1400/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 1500/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 1600/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 1700/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 1800/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 1900/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 2000/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 2100/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 2200/3438 dates\n",
      "19:49:09 - DataPipeline - INFO - Processed 2300/3438 dates\n",
      "19:49:10 - DataPipeline - INFO - Processed 2400/3438 dates\n",
      "19:49:10 - DataPipeline - INFO - Processed 2500/3438 dates\n",
      "19:49:10 - DataPipeline - INFO - Processed 2600/3438 dates\n",
      "19:49:10 - DataPipeline - INFO - Processed 2700/3438 dates\n",
      "19:49:10 - DataPipeline - INFO - Processed 2800/3438 dates\n",
      "19:49:10 - DataPipeline - INFO - Processed 2900/3438 dates\n",
      "19:49:10 - DataPipeline - INFO - Processed 3000/3438 dates\n",
      "19:49:10 - DataPipeline - INFO - Processed 3100/3438 dates\n",
      "19:49:10 - DataPipeline - INFO - Processed 3200/3438 dates\n",
      "19:49:10 - DataPipeline - INFO - Processed 3300/3438 dates\n",
      "19:49:10 - DataPipeline - INFO - Processed 3400/3438 dates\n",
      "19:49:10 - DataPipeline - INFO - Normalization complete\n",
      "19:49:10 - DataPipeline - INFO - Feature preparation complete\n",
      "19:49:10 - DataPipeline - INFO - Features shape: (47411, 1180)\n",
      "19:49:10 - DataPipeline - INFO - Targets shape: 47411\n",
      "19:49:10 - DataPipeline - INFO - Checking index alignment...\n",
      "19:49:10 - DataPipeline - INFO - Sorting indices...\n",
      "19:49:11 - DataPipeline - INFO - Calculating split point...\n",
      "19:49:11 - DataPipeline - INFO - Total unique dates: 3438\n",
      "19:49:11 - DataPipeline - INFO - Split date calculated: 2021-03-31 00:00:00-04:00\n",
      "19:49:11 - DataPipeline - INFO - Splitting data...\n",
      "19:49:11 - DataPipeline - INFO - X_train split complete\n",
      "19:49:11 - DataPipeline - INFO - X_test split complete\n",
      "19:49:11 - DataPipeline - INFO - y_train split complete\n",
      "19:49:11 - DataPipeline - INFO - y_test split complete\n",
      "19:49:11 - DataPipeline - INFO - Training data from 2010-04-28 00:00:00-04:00 to 2021-03-31 00:00:00-04:00\n",
      "19:49:11 - DataPipeline - INFO - Testing data from 2021-03-31 00:00:00-04:00 to 2023-12-21 00:00:00-05:00\n",
      "19:49:11 - DataPipeline - INFO - Training samples: 37793, Testing samples: 9618\n",
      "\n",
      "I got the training data with shapes: (37793, 1180) and (9618, 1180)\n",
      "\n",
      "X_train shape: (37793, 1180)\n",
      "X_test shape: (9618, 1180)\n"
     ]
    }
   ],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = DataPipeline(\n",
    "    start_date='2010-01-01',\n",
    "    end_date='2023-12-31',\n",
    "    universe_size=500,\n",
    "    cache_dir='data/cache',\n",
    "    price_col='Close'\n",
    ")\n",
    "\n",
    "# Fetch data\n",
    "symbols = [\n",
    "    'AMZN', 'META', 'NVDA', 'TSLA', 'JPM', 'JNJ', 'WMT', \n",
    "    'PG', 'XOM', 'BAC', 'HD', 'COST', 'V', 'DIS'\n",
    "]\n",
    "pipeline.fetch_data(symbols)\n",
    "\n",
    "# Check status after fetch\n",
    "print(\"\\nStatus after fetch:\")\n",
    "print(format_data_status(pipeline))\n",
    "\n",
    "# Process data\n",
    "pipeline.process_data()\n",
    "\n",
    "# Check quality after processing\n",
    "print(\"\\nQuality after processing:\")\n",
    "print(format_data_quality(pipeline))\n",
    "\n",
    "# Create universe\n",
    "pipeline.create_universe()\n",
    "\n",
    "print(\"\\nStatus after creating universe:\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = pipeline.get_training_data()\n",
    "\n",
    "# Check status after creating universe\n",
    "print(f\"\\nI got the training data with shapes: {X_train.shape} and {X_test.shape}\")\n",
    "\n",
    "print(\"\\nX_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:50:07 - DataPipeline - INFO - Filtered universe contains 4 symbols\n",
      "Mean Reversion Positions:\n",
      "----------------------------------------\n",
      "Long positions (0): []\n",
      "Short positions (0): []\n"
     ]
    }
   ],
   "source": [
    "from models.mean_reversion_analyzer import MeanReversionAnalyzer\n",
    "\n",
    "# Initialize mean reversion analyzer with our existing pipeline\n",
    "mean_reversion = MeanReversionAnalyzer(\n",
    "    data_pipeline=pipeline,\n",
    "    lookback_periods=20,\n",
    "    z_score_threshold=2.0,\n",
    "    volume_percentile=0.7,\n",
    "    max_positions=5\n",
    ")\n",
    "\n",
    "# Generate signals and store them\n",
    "signals = mean_reversion.generate_signals()\n",
    "\n",
    "# Display initial results\n",
    "print(\"Mean Reversion Positions:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Long positions ({len(signals.longs)}):\", signals.longs)\n",
    "print(f\"Short positions ({len(signals.shorts)}):\", signals.shorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current universe: ['NVDA', 'BAC', 'TSLA', 'AMZN', 'META', 'WMT', 'JPM', 'XOM', 'V', 'DIS', 'PG', 'JNJ', 'HD', 'COST']\n",
      "\n",
      "Filtered universe: ['NVDA', 'BAC', 'TSLA', 'AMZN']\n",
      "\n",
      "Z-scores for NVDA:\n",
      "Latest z-score: 1.028\n",
      "\n",
      "Z-scores for BAC:\n",
      "Latest z-score: 0.918\n",
      "\n",
      "Z-scores for TSLA:\n",
      "Latest z-score: 0.170\n",
      "\n",
      "Z-scores for AMZN:\n",
      "Latest z-score: 0.619\n",
      "\n",
      "Z-scores for META:\n",
      "Latest z-score: 1.057\n",
      "\n",
      "Z-scores for WMT:\n",
      "Latest z-score: 1.539\n",
      "\n",
      "Z-scores for JPM:\n",
      "Latest z-score: 1.320\n",
      "\n",
      "Z-scores for XOM:\n",
      "Latest z-score: -0.535\n",
      "\n",
      "Z-scores for V:\n",
      "Latest z-score: 1.074\n",
      "\n",
      "Z-scores for DIS:\n",
      "Latest z-score: -1.341\n",
      "\n",
      "Z-scores for PG:\n",
      "Latest z-score: 0.044\n",
      "\n",
      "Z-scores for JNJ:\n",
      "Latest z-score: 0.474\n",
      "\n",
      "Z-scores for HD:\n",
      "Latest z-score: 0.545\n",
      "\n",
      "Z-scores for COST:\n",
      "Latest z-score: 0.984\n"
     ]
    }
   ],
   "source": [
    "# First, let's check what our universe looks like\n",
    "print(\"Current universe:\", pipeline.universe)\n",
    "\n",
    "# Let's examine the filtering process\n",
    "filtered_universe = mean_reversion.filter_universe()\n",
    "print(\"\\nFiltered universe:\", filtered_universe)\n",
    "\n",
    "# Let's look at the actual z-scores before thresholding\n",
    "for symbol in pipeline.universe:\n",
    "    data = pipeline.processed_data[symbol]\n",
    "    z_scores = mean_reversion.calculate_z_scores(data)\n",
    "    print(f\"\\nZ-scores for {symbol}:\")\n",
    "    print(f\"Latest z-score: {z_scores.iloc[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:51:06 - DataPipeline - INFO - Initializing model pipeline with xgboost model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\calvi\\anaconda3\\envs\\ns_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\calvi\\anaconda3\\envs\\ns_env\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:51:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"early_stopping_rounds\", \"n_estimators\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:51:11 - DataPipeline - INFO - Model Evaluation Results:\n",
      "19:51:11 - DataPipeline - INFO - mse: 0.0023\n",
      "19:51:11 - DataPipeline - INFO - rmse: 0.0476\n",
      "19:51:11 - DataPipeline - INFO - mae: 0.0332\n",
      "19:51:11 - DataPipeline - INFO - r2: -0.0019\n",
      "19:51:11 - DataPipeline - INFO - directional_accuracy: 0.5331\n",
      "19:51:11 - DataPipeline - INFO - precision: 0.5335\n",
      "19:51:11 - DataPipeline - INFO - recall: 0.9934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mse': 0.0022635396183796775,\n",
       " 'rmse': 0.047576670946795734,\n",
       " 'mae': 0.033179645302192154,\n",
       " 'r2': -0.0019390927886824283,\n",
       " 'directional_accuracy': 0.5330630068621335,\n",
       " 'precision': 0.5334938245760937,\n",
       " 'recall': 0.9933736113817969}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "from models.model_pipeline import ModelPipeline\n",
    "\n",
    "# Initialize pipeline\n",
    "model = ModelPipeline()\n",
    "\n",
    "# Train model\n",
    "model.train(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "model.evaluate_model(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ns_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
